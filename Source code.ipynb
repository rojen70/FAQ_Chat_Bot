{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aeee62d-d8a3-4d65-b745-10ea91cb53e2",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "007d34d7-be48-49c5-a738-f3449efe0fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\pydantic\\_internal\\_fields.py:132: UserWarning: Field \"model_url\" in LlamaCPP has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\asus\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\pydantic\\_internal\\_fields.py:132: UserWarning: Field \"model_path\" in LlamaCPP has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\asus\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\pydantic\\_internal\\_fields.py:132: UserWarning: Field \"model_kwargs\" in LlamaCPP has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\asus\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch # Pytorch library for deep learning and tensor operations\n",
    "import pandas as pd #Library for data manipulation and analysis\n",
    "import wget # For downloading files from the internet\n",
    "\n",
    "# Import modules from LlamaIndex for managing and quering indices\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.llms.llama_cpp import LlamaCPP # For Llama model usage via C++ bindings\n",
    "from llama_index.llms.llama_cpp.llama_utils import (\n",
    "    messages_to_prompt, # Converts structured messages to a prompt string\n",
    "    completion_to_prompt, # Converts a model's completion to a prompt format\n",
    ")\n",
    "\n",
    "# Additional imports from LlamaIndex for building and managing indices\n",
    "from llama_index.core import (\n",
    "  SimpleDirectoryReader,\n",
    "  VectorStoreIndex,\n",
    "  ServiceContext,\n",
    ")\n",
    "\n",
    "# Set global tokenizer for LlamaIndex\n",
    "from llama_index.core import set_global_tokenizer\n",
    "\n",
    "# Transformers library for pre-trained model tokenization and embedding\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Embedding utilities for LlamaIndex\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding # Embedding models from HuggingFace\n",
    "\n",
    "# Importing SentenceTransformers for advanced sentence embeddings\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Metrics from sklearn for evaluation\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# Natural Language Toolkit (NLTK) for tokenization and BLEU score calculation\n",
    "import nltk  # Standard NLP library\n",
    "from nltk.tokenize import word_tokenize  # Tokenizer for splitting sentences into words\n",
    "from nltk.translate.bleu_score import (\n",
    "    sentence_bleu,  # Calculate BLEU for individual sentences\n",
    "    corpus_bleu,   # Calculate BLEU for a corpus\n",
    "    SmoothingFunction # Adds smoothing to BLEU calculations\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e36af12-bc05-4d80-a75c-35c1a3105576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0c361c-ecce-42b3-8bd8-b84b646da0e2",
   "metadata": {},
   "source": [
    "### Download model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14790784-aa38-404a-9e4a-71b9ee45b104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'llama-2-7b-chat.Q2_K.gguf'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create function to download llama 2 model from hugging face to local machine for faster computation\n",
    "def bar_custom(current, total, width=80):\n",
    "    print(\"Downloading %d%% [%d/%d] bytes\" % (current/total * 100, current, total))\n",
    "\n",
    "model_url = \"https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q2_K.gguf\"\n",
    "wget.download(model_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8296d4be-54fe-4c54-81a1-01cb35b3202a",
   "metadata": {},
   "source": [
    "### Wrapping Llama 2 in LlamaCPP\n",
    "Wrapping the LLaMA 2 model in a LlamaCPP mode offers several significant benefits. These revolve around perfomance optimization, ease of integration and flexibility when working with large langugage models. Some benefits are:\n",
    "1. Performance optimization\n",
    "2. Scalability\n",
    "3. Integration Flexibility\n",
    "4. Model Efficiency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65d60640-3e33-4fcf-8d41-f7624ac9dc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_llm() -> LlamaCPP:\n",
    "    \"\"\"\n",
    "    Initializes and returns a LlamaCPP instance with specific configurations.\n",
    "\n",
    "    Returns:\n",
    "        LlamaCPP: Configured instance of the LlamaCPP language model.\n",
    "    \"\"\"\n",
    "    return LlamaCPP(\n",
    "    model_path=r\"C:\\Users\\asus\\llama-2-7b-chat.Q2_K.gguf\", # Path to the LLaMA model file\n",
    "    temperature=0.1, # Sampling temperature (Controls the randomness in generated text)\n",
    "    max_new_tokens=500, # Maximum number of tokens to generate in a single response\n",
    "    context_window=3900, # Size of the model's context window (maximum input length in tokens) \n",
    "    generate_kwargs={}, # Additional keyword arguments for text generation\n",
    "    model_kwargs={\"n_gpu_layers\":1}, # Model-specific arguments, such as GPU layer allocation\n",
    "    messages_to_prompt=messages_to_prompt, # Function to convert structured messages into a prompt\n",
    "    completion_to_prompt=completion_to_prompt, # Function to format the model's output as a prompt\n",
    "    verbose=True, # Enables verbose logging for debugging and monitoring\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e817b27a-2695-4618-9e10-09943ace5a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the global tokenizer for the LlamaIndex framework.\n",
    "# This ensures that all text tokenization is handled using the tokenizer\n",
    "# associated with the specified LLaMA-2 model from HuggingFace.\n",
    "set_global_tokenizer(\n",
    "    AutoTokenizer.from_pretrained(\"NousResearch/Llama-2-7b-chat-hf\").encode\n",
    "     # Load the pre-trained tokenizer for \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "    # and use its `encode` method to tokenize text into numerical IDs.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "befc69d3-47e6-4697-b5ed-6a86a0cb98ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an embedding model for converting text into vector representations.\n",
    "# \"HuggingFaceEmbedding\" is used to create embeddings compatible with LlamaIndex.\n",
    "# The model \"BAAI/bge-small-en-v1.5\" is a pre-trained small English embedding model\n",
    "# available from HuggingFace, designed to generate dense vector representations of text.\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80713097-fe40-4ea7-b08e-ef3a88a8b4bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from C:\\Users\\asus\\llama-2-7b-chat.Q2_K.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 10\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q2_K:   65 tensors\n",
      "llama_model_loader: - type q3_K:  160 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.1684 MB\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q2_K - Medium\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 2.63 GiB (3.35 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.14 MiB\n",
      "llm_load_tensors:        CPU buffer size =  2694.32 MiB\n",
      ".................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 3904\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  1952.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1952.00 MiB, K (f16):  976.00 MiB, V (f16):  976.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   283.63 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.name': 'LLaMA v2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '11008', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '10', 'llama.attention.head_count_kv': '32', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0'}\n",
      "Using fallback chat format: llama-2\n"
     ]
    }
   ],
   "source": [
    "# create a CPP model instance\n",
    "llm = select_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "05c303f7-dc21-401b-898e-bbd0d18bec70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file from: C:\\Downloads\\Rules and Conditions _ McDonald's Monopoly 2024 IE.pdf\n"
     ]
    }
   ],
   "source": [
    "# path file for the document to be looked up for RAG\n",
    "file_path = r\"C:\\Downloads\\Rules and Conditions _ McDonald's Monopoly 2024 IE.pdf\"\n",
    "print(f\"Loading file from: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "87f95c88-56f3-4f14-8653-e70f73802da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load documents\n",
    "documents = SimpleDirectoryReader(input_files=\n",
    "    [r\"C:\\Users\\asus\\Downloads\\Rules and Conditions _ McDonald's Monopoly 2024 IE.pdf\"]\n",
    ").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b6bac4a0-ae9a-4df4-93b7-1f5666f64be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vector store index\n",
    "index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "420bbcb9-f6ae-47f6-ba44-45420e096d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up query engine\n",
    "query_engine = index.as_query_engine(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7594db10-682a-4158-bf8b-8d57725a7c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get query and response sample dataset from the policy document for evaluation\n",
    "dataset_path = r\"C:\\Users\\asus\\Desktop\\mcdonalds_monopoly_dataset.csv\"\n",
    "data = pd.read_csv(dataset_path)\n",
    "\n",
    "queries = data['Query'].tolist()\n",
    "expected_responses = data['Response'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "63fff109-629d-4364-9760-8da4e315eee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who is eligible to participate in the promotion?</td>\n",
       "      <td>Entrants must be aged 18 or over and residents...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the dates of the promotion?</td>\n",
       "      <td>The promotion runs from September 4, 2024, to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How can I enter the promotion?</td>\n",
       "      <td>To enter, purchase a qualifying menu item to r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are the categories of prizes?</td>\n",
       "      <td>Prizes include Instant Win, Collect to Win, an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the deadline for submitting prize claims?</td>\n",
       "      <td>All prize claims must be submitted by October ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Query  \\\n",
       "0   Who is eligible to participate in the promotion?   \n",
       "1               What are the dates of the promotion?   \n",
       "2                     How can I enter the promotion?   \n",
       "3                 What are the categories of prizes?   \n",
       "4  What is the deadline for submitting prize claims?   \n",
       "\n",
       "                                            Response  \n",
       "0  Entrants must be aged 18 or over and residents...  \n",
       "1  The promotion runs from September 4, 2024, to ...  \n",
       "2  To enter, purchase a qualifying menu item to r...  \n",
       "3  Prizes include Instant Win, Collect to Win, an...  \n",
       "4  All prize claims must be submitted by October ...  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79458aca-30ac-439a-a7f0-cb37a84fe693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained SentenceTransformer model for computing semantic similarity\n",
    "# \"all-MiniLM-L6-v2\" is a small, efficient model that generates embeddings for text\n",
    "# suitable for tasks like sentence-level similarity comparison.\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def evaluate_responses(model, queries, expected_responses):\n",
    "    \"\"\"\n",
    "    Evaluate the model's responses to queries by calculating semantic similarity with expected responses.\n",
    "    \n",
    "    Args:\n",
    "        model: The model used for generating responses (e.g., a query engine).\n",
    "        queries: A list of input queries/questions.\n",
    "        expected_responses: A list of expected responses to those queries.\n",
    "        \n",
    "    Returns:\n",
    "        None (prints average semantic similarity).\n",
    "    \"\"\"\n",
    "    precision_scores = [] # List to store precision scores \n",
    "    semantic_similarities = [] # List to store semantic similarity values between model responses and expected responses\n",
    "    # Iterate over each query and its expected response\n",
    "    for query, expected in zip(queries, expected_responses):\n",
    "        # Get model's response\n",
    "        response = query_engine.query(query)  \n",
    "        # Compute semantic similarity\n",
    "        #Encode both the model's response and the expected response into embeddings\n",
    "        embeddings = embedder.encode([response, expected])\n",
    "        #Calculate the cosine similarity between the embeddings of the response and the expected response\n",
    "        similarity = util.cos_sim(embeddings[0], embeddings[1]).item()\n",
    "        semantic_similarities.append(similarity)\n",
    "\n",
    "        \n",
    "\n",
    "    # Aggregate metrics\n",
    "    avg_similarity = sum(semantic_similarities) / len(semantic_similarities)\n",
    "    print(f\"Average Semantic Similarity: {avg_similarity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e7bdc2d8-5840-4dee-817e-2bd2e2fd8c67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\llama_cpp\\llama.py:1138: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n",
      "  warnings.warn(\n",
      "Llama.generate: 76 prefix-match hit, remaining 1327 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =   15842.58 ms\n",
      "llama_print_timings:      sample time =       3.71 ms /    91 runs   (    0.04 ms per token, 24515.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =   42600.02 ms /  1327 tokens (   32.10 ms per token,    31.15 tokens per second)\n",
      "llama_print_timings:        eval time =    9813.79 ms /    90 runs   (  109.04 ms per token,     9.17 tokens per second)\n",
      "llama_print_timings:       total time =   52478.43 ms /  1417 tokens\n",
      "Llama.generate: 76 prefix-match hit, remaining 1309 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =   15842.58 ms\n",
      "llama_print_timings:      sample time =       4.09 ms /   101 runs   (    0.04 ms per token, 24664.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =   42583.59 ms /  1309 tokens (   32.53 ms per token,    30.74 tokens per second)\n",
      "llama_print_timings:        eval time =   10527.01 ms /   100 runs   (  105.27 ms per token,     9.50 tokens per second)\n",
      "llama_print_timings:       total time =   53175.51 ms /  1409 tokens\n",
      "Llama.generate: 76 prefix-match hit, remaining 1726 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =   15842.58 ms\n",
      "llama_print_timings:      sample time =       7.89 ms /   199 runs   (    0.04 ms per token, 25225.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =   57472.82 ms /  1726 tokens (   33.30 ms per token,    30.03 tokens per second)\n",
      "llama_print_timings:        eval time =   23251.19 ms /   198 runs   (  117.43 ms per token,     8.52 tokens per second)\n",
      "llama_print_timings:       total time =   80881.42 ms /  1924 tokens\n",
      "Llama.generate: 76 prefix-match hit, remaining 1700 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =   15842.58 ms\n",
      "llama_print_timings:      sample time =      20.02 ms /   500 runs   (    0.04 ms per token, 24971.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =   56333.53 ms /  1700 tokens (   33.14 ms per token,    30.18 tokens per second)\n",
      "llama_print_timings:        eval time =   58477.05 ms /   499 runs   (  117.19 ms per token,     8.53 tokens per second)\n",
      "llama_print_timings:       total time =  115348.62 ms /  2199 tokens\n",
      "Llama.generate: 77 prefix-match hit, remaining 1370 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =   15842.58 ms\n",
      "llama_print_timings:      sample time =       5.54 ms /   137 runs   (    0.04 ms per token, 24720.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =   44758.05 ms /  1370 tokens (   32.67 ms per token,    30.61 tokens per second)\n",
      "llama_print_timings:        eval time =   14575.26 ms /   136 runs   (  107.17 ms per token,     9.33 tokens per second)\n",
      "llama_print_timings:       total time =   59426.20 ms /  1506 tokens\n",
      "Llama.generate: 76 prefix-match hit, remaining 1299 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =   15842.58 ms\n",
      "llama_print_timings:      sample time =      11.85 ms /   289 runs   (    0.04 ms per token, 24386.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =   43294.51 ms /  1299 tokens (   33.33 ms per token,    30.00 tokens per second)\n",
      "llama_print_timings:        eval time =   30996.27 ms /   288 runs   (  107.63 ms per token,     9.29 tokens per second)\n",
      "llama_print_timings:       total time =   74530.44 ms /  1587 tokens\n",
      "Llama.generate: 76 prefix-match hit, remaining 1619 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =   15842.58 ms\n",
      "llama_print_timings:      sample time =       2.50 ms /    55 runs   (    0.05 ms per token, 22017.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =   57020.83 ms /  1619 tokens (   35.22 ms per token,    28.39 tokens per second)\n",
      "llama_print_timings:        eval time =    7485.26 ms /    54 runs   (  138.62 ms per token,     7.21 tokens per second)\n",
      "llama_print_timings:       total time =   64548.53 ms /  1673 tokens\n",
      "Llama.generate: 76 prefix-match hit, remaining 1959 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =   15842.58 ms\n",
      "llama_print_timings:      sample time =      14.78 ms /   344 runs   (    0.04 ms per token, 23274.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =   82455.13 ms /  1959 tokens (   42.09 ms per token,    23.76 tokens per second)\n",
      "llama_print_timings:        eval time =   48902.84 ms /   343 runs   (  142.57 ms per token,     7.01 tokens per second)\n",
      "llama_print_timings:       total time =  131715.64 ms /  2302 tokens\n",
      "Llama.generate: 76 prefix-match hit, remaining 1843 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =   15842.58 ms\n",
      "llama_print_timings:      sample time =       2.33 ms /    54 runs   (    0.04 ms per token, 23195.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =   70098.39 ms /  1843 tokens (   38.03 ms per token,    26.29 tokens per second)\n",
      "llama_print_timings:        eval time =    6910.63 ms /    53 runs   (  130.39 ms per token,     7.67 tokens per second)\n",
      "llama_print_timings:       total time =   77047.89 ms /  1896 tokens\n",
      "Llama.generate: 76 prefix-match hit, remaining 1656 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =   15842.58 ms\n",
      "llama_print_timings:      sample time =       9.22 ms /   215 runs   (    0.04 ms per token, 23313.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =   61875.96 ms /  1656 tokens (   37.36 ms per token,    26.76 tokens per second)\n",
      "llama_print_timings:        eval time =   27660.11 ms /   214 runs   (  129.25 ms per token,     7.74 tokens per second)\n",
      "llama_print_timings:       total time =   89715.68 ms /  1870 tokens\n",
      "Llama.generate: 76 prefix-match hit, remaining 1892 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =   15842.58 ms\n",
      "llama_print_timings:      sample time =       6.17 ms /   146 runs   (    0.04 ms per token, 23678.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =   72299.31 ms /  1892 tokens (   38.21 ms per token,    26.17 tokens per second)\n",
      "llama_print_timings:        eval time =   19874.26 ms /   145 runs   (  137.06 ms per token,     7.30 tokens per second)\n",
      "llama_print_timings:       total time =   92287.26 ms /  2037 tokens\n",
      "Llama.generate: 76 prefix-match hit, remaining 1658 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =   15842.58 ms\n",
      "llama_print_timings:      sample time =      11.20 ms /   213 runs   (    0.05 ms per token, 19026.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =   64889.02 ms /  1658 tokens (   39.14 ms per token,    25.55 tokens per second)\n",
      "llama_print_timings:        eval time =   28684.06 ms /   212 runs   (  135.30 ms per token,     7.39 tokens per second)\n",
      "llama_print_timings:       total time =   93772.43 ms /  1870 tokens\n",
      "Llama.generate: 76 prefix-match hit, remaining 1688 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =   15842.58 ms\n",
      "llama_print_timings:      sample time =      10.90 ms /   212 runs   (    0.05 ms per token, 19440.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =   63943.97 ms /  1688 tokens (   37.88 ms per token,    26.40 tokens per second)\n",
      "llama_print_timings:        eval time =   28302.66 ms /   211 runs   (  134.14 ms per token,     7.46 tokens per second)\n",
      "llama_print_timings:       total time =   92445.96 ms /  1899 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Semantic Similarity: 0.68\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the semantic similarity of the model \n",
    "evaluate_responses(llm,queries,expected_responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2467f265-288f-45bb-94f1-bb4b3b3a3912",
   "metadata": {},
   "source": [
    "A semantic score of 0.68 was achieved "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b900e27a-5891-4b5b-b836-c7a360c88ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the tokenizer model\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d2424a38-3c2f-436f-934d-ed56d658b1bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 76 prefix-match hit, remaining 1327 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =   15990.94 ms\n",
      "llama_print_timings:      sample time =       4.44 ms /   105 runs   (    0.04 ms per token, 23627.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =   43208.98 ms /  1327 tokens (   32.56 ms per token,    30.71 tokens per second)\n",
      "llama_print_timings:        eval time =   11357.63 ms /   104 runs   (  109.21 ms per token,     9.16 tokens per second)\n",
      "llama_print_timings:       total time =   54640.39 ms /  1431 tokens\n",
      "Llama.generate: 76 prefix-match hit, remaining 1309 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =   15990.94 ms\n",
      "llama_print_timings:      sample time =       2.75 ms /    68 runs   (    0.04 ms per token, 24754.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =   42394.71 ms /  1309 tokens (   32.39 ms per token,    30.88 tokens per second)\n",
      "llama_print_timings:        eval time =    7478.80 ms /    67 runs   (  111.62 ms per token,     8.96 tokens per second)\n",
      "llama_print_timings:       total time =   49915.12 ms /  1376 tokens\n",
      "Llama.generate: 76 prefix-match hit, remaining 1726 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =   15990.94 ms\n",
      "llama_print_timings:      sample time =       8.04 ms /   195 runs   (    0.04 ms per token, 24268.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =   57290.39 ms /  1726 tokens (   33.19 ms per token,    30.13 tokens per second)\n",
      "llama_print_timings:        eval time =   22684.95 ms /   194 runs   (  116.93 ms per token,     8.55 tokens per second)\n",
      "llama_print_timings:       total time =   80127.15 ms /  1920 tokens\n",
      "Llama.generate: 76 prefix-match hit, remaining 1700 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =   15990.94 ms\n",
      "llama_print_timings:      sample time =      19.54 ms /   500 runs   (    0.04 ms per token, 25587.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =   57169.61 ms /  1700 tokens (   33.63 ms per token,    29.74 tokens per second)\n",
      "llama_print_timings:        eval time =   58255.14 ms /   499 runs   (  116.74 ms per token,     8.57 tokens per second)\n",
      "llama_print_timings:       total time =  115938.78 ms /  2199 tokens\n",
      "Llama.generate: 77 prefix-match hit, remaining 1370 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =   15990.94 ms\n",
      "llama_print_timings:      sample time =       5.24 ms /   129 runs   (    0.04 ms per token, 24632.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =   45774.49 ms /  1370 tokens (   33.41 ms per token,    29.93 tokens per second)\n",
      "llama_print_timings:        eval time =   14196.17 ms /   128 runs   (  110.91 ms per token,     9.02 tokens per second)\n",
      "llama_print_timings:       total time =   60058.52 ms /  1498 tokens\n",
      "Llama.generate: 76 prefix-match hit, remaining 1299 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =   15990.94 ms\n",
      "llama_print_timings:      sample time =      12.11 ms /   293 runs   (    0.04 ms per token, 24192.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =   43065.04 ms /  1299 tokens (   33.15 ms per token,    30.16 tokens per second)\n",
      "llama_print_timings:        eval time =   32245.62 ms /   292 runs   (  110.43 ms per token,     9.06 tokens per second)\n",
      "llama_print_timings:       total time =   75561.26 ms /  1591 tokens\n",
      "Llama.generate: 76 prefix-match hit, remaining 1619 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =   15990.94 ms\n",
      "llama_print_timings:      sample time =       4.06 ms /   104 runs   (    0.04 ms per token, 25641.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =   54322.64 ms /  1619 tokens (   33.55 ms per token,    29.80 tokens per second)\n",
      "llama_print_timings:        eval time =   11453.93 ms /   103 runs   (  111.20 ms per token,     8.99 tokens per second)\n",
      "llama_print_timings:       total time =   65845.62 ms /  1722 tokens\n",
      "Llama.generate: 76 prefix-match hit, remaining 1959 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =   15990.94 ms\n",
      "llama_print_timings:      sample time =      11.82 ms /   299 runs   (    0.04 ms per token, 25289.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =   67337.73 ms /  1959 tokens (   34.37 ms per token,    29.09 tokens per second)\n",
      "llama_print_timings:        eval time =   35853.49 ms /   298 runs   (  120.31 ms per token,     8.31 tokens per second)\n",
      "llama_print_timings:       total time =  103447.30 ms /  2257 tokens\n",
      "Llama.generate: 76 prefix-match hit, remaining 1843 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =   15990.94 ms\n",
      "llama_print_timings:      sample time =       2.14 ms /    54 runs   (    0.04 ms per token, 25186.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =   62418.33 ms /  1843 tokens (   33.87 ms per token,    29.53 tokens per second)\n",
      "llama_print_timings:        eval time =    6030.80 ms /    53 runs   (  113.79 ms per token,     8.79 tokens per second)\n",
      "llama_print_timings:       total time =   68482.71 ms /  1896 tokens\n",
      "Llama.generate: 76 prefix-match hit, remaining 1656 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =   15990.94 ms\n",
      "llama_print_timings:      sample time =       8.17 ms /   196 runs   (    0.04 ms per token, 24004.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =   55791.56 ms /  1656 tokens (   33.69 ms per token,    29.68 tokens per second)\n",
      "llama_print_timings:        eval time =   22606.27 ms /   195 runs   (  115.93 ms per token,     8.63 tokens per second)\n",
      "llama_print_timings:       total time =   78546.47 ms /  1851 tokens\n",
      "Llama.generate: 76 prefix-match hit, remaining 1892 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =   15990.94 ms\n",
      "llama_print_timings:      sample time =       5.91 ms /   142 runs   (    0.04 ms per token, 24018.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =   64429.28 ms /  1892 tokens (   34.05 ms per token,    29.37 tokens per second)\n",
      "llama_print_timings:        eval time =   17282.16 ms /   141 runs   (  122.57 ms per token,     8.16 tokens per second)\n",
      "llama_print_timings:       total time =   81814.51 ms /  2033 tokens\n",
      "Llama.generate: 76 prefix-match hit, remaining 1658 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =   15990.94 ms\n",
      "llama_print_timings:      sample time =       8.25 ms /   205 runs   (    0.04 ms per token, 24854.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =   56005.28 ms /  1658 tokens (   33.78 ms per token,    29.60 tokens per second)\n",
      "llama_print_timings:        eval time =   23625.02 ms /   204 runs   (  115.81 ms per token,     8.63 tokens per second)\n",
      "llama_print_timings:       total time =   79785.48 ms /  1862 tokens\n",
      "Llama.generate: 76 prefix-match hit, remaining 1688 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =   15990.94 ms\n",
      "llama_print_timings:      sample time =       8.53 ms /   209 runs   (    0.04 ms per token, 24504.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =   56978.88 ms /  1688 tokens (   33.76 ms per token,    29.63 tokens per second)\n",
      "llama_print_timings:        eval time =   24314.44 ms /   208 runs   (  116.90 ms per token,     8.55 tokens per second)\n",
      "llama_print_timings:       total time =   81451.33 ms /  1896 tokens\n"
     ]
    }
   ],
   "source": [
    "# Create a list of all the response that would be generated by the model for the given sample query\n",
    "generated_responses = []\n",
    "for query in queries:\n",
    "    response = query_engine.query(query)\n",
    "    generated_responses.append(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "51504114-163f-45ec-94b3-03c836717e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.0022\n",
      "BLEU Score: 0.0060\n",
      "BLEU Score: 0.0016\n",
      "BLEU Score: 0.0007\n",
      "BLEU Score: 0.0023\n",
      "BLEU Score: 0.0010\n",
      "BLEU Score: 0.0025\n",
      "BLEU Score: 0.0009\n",
      "BLEU Score: 0.0056\n",
      "BLEU Score: 0.0015\n",
      "BLEU Score: 0.0020\n",
      "BLEU Score: 0.0013\n",
      "BLEU Score: 0.0013\n",
      "Corpus BLEU Score: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\asus\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\asus\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "# Example Data\n",
    "tokenized_reference_responses =  [word_tokenize(response.lower()) for response in expected_responses]\n",
    "tokenized_generated_responses =  [word_tokenize(response.lower()) for response in generated_responses]\n",
    "\n",
    "smooth_fn = SmoothingFunction().method1\n",
    "# Calculate Sentence-Level BLEU\n",
    "for ref, gen in zip(tokenized_reference_responses, tokenized_generated_responses):\n",
    "    score = sentence_bleu(ref, gen, smoothing_function=smooth_fn)\n",
    "    print(f\"BLEU Score: {score:.4f}\")\n",
    "\n",
    "# Calculate Corpus-Level BLEU\n",
    "corpus_score = corpus_bleu(tokenized_reference_responses, tokenized_generated_responses)\n",
    "print(f\"Corpus BLEU Score: {corpus_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "215b3d3b-6fbf-439e-90a8-2f969dd5c72a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.py,\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py,\n",
    "import streamlit as st \n",
    "from llama_index.core import (\n",
    "  SimpleDirectoryReader,\n",
    "  VectorStoreIndex,\n",
    "  ServiceContext,\n",
    ")\n",
    "\n",
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "from llama_index.llms.llama_cpp.llama_utils import (\n",
    "    messages_to_prompt,\n",
    "    completion_to_prompt,\n",
    ")\n",
    "from langchain.schema import(SystemMessage, HumanMessage, AIMessage)\n",
    "\n",
    "from llama_index.core import set_global_tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "set_global_tokenizer(\n",
    "    AutoTokenizer.from_pretrained(\"NousResearch/Llama-2-7b-chat-hf\").encode\n",
    ")\n",
    "\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "# load documents\n",
    "documents = SimpleDirectoryReader(input_files=\n",
    "    [r\"C:\\Users\\asus\\Downloads\\Rules and Conditions _ McDonald's Monopoly 2024 IE.pdf\"]\n",
    ").load_data()\n",
    "\n",
    "# create vector store index\n",
    "index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)\n",
    "\n",
    "\n",
    "def init_page() -> None:\n",
    "  st.set_page_config(\n",
    "    page_title=\"Personal Chatbot\"\n",
    "  )\n",
    "  st.header(\"Personal Chatbot\")\n",
    "  st.sidebar.title(\"Options\")\n",
    "\n",
    "def select_llm() -> LlamaCPP:\n",
    "  return LlamaCPP(\n",
    "    model_path=\"llama-2-7b-chat.Q2_K.gguf\",\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=500,\n",
    "    context_window=3900,\n",
    "    generate_kwargs={},\n",
    "    model_kwargs={\"n_gpu_layers\":1},\n",
    "    messages_to_prompt=messages_to_prompt,\n",
    "    completion_to_prompt=completion_to_prompt,\n",
    "    verbose=True,\n",
    "  )\n",
    "\n",
    "def init_messages() -> None:\n",
    "  clear_button = st.sidebar.button(\"Clear Conversation\", key=\"clear\")\n",
    "  if clear_button or \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = [\n",
    "      SystemMessage(\n",
    "        content=\"you are a helpful AI assistant. Reply your answer in markdown format.\"\n",
    "      )\n",
    "    ]\n",
    "\n",
    "def get_answer(llm, messages) -> str:\n",
    "    query_engine = index.as_query_engine(llm=llm)\n",
    "    response = query_engine.query(messages)\n",
    "    return response\n",
    "\n",
    "def main() -> None:\n",
    "  init_page()\n",
    "  llm = select_llm()\n",
    "  init_messages()\n",
    "\n",
    "  if user_input := st.chat_input(\"Input your question!\"):\n",
    "    st.session_state.messages.append(HumanMessage(content=user_input))\n",
    "    with st.spinner(\"Bot is typing ...\"):\n",
    "      answer = get_answer(llm, user_input)\n",
    "      print(answer)\n",
    "    st.session_state.messages.append(AIMessage(content=str(answer)))\n",
    "    \n",
    "\n",
    "  messages = st.session_state.get(\"messages\", [])\n",
    "  for message in messages:\n",
    "    if isinstance(message, AIMessage):\n",
    "      with st.chat_message(\"assistant\"):\n",
    "        st.markdown(message.content)\n",
    "    elif isinstance(message, HumanMessage):\n",
    "      with st.chat_message(\"user\"):\n",
    "        st.markdown(message.content)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f26b3a-05a2-40c5-af80-abfe7ebcb7e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
